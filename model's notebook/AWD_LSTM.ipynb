{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/tommasomncttn/NLP-Disaster-Tweet-Detection/blob/main/model's%20notebook/AWD_LSTM.ipynb",
      "authorship_tag": "ABX9TyMVNapdMqQYTTCjk3SAvA+o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tommasomncttn/NLP-Disaster-Tweet-Detection/blob/main/model's%20notebook/AWD_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "RVF8LuQNyffi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEeEafW1yYJ7"
      },
      "outputs": [],
      "source": [
        "# ===========================================\n",
        "# ||                                       ||\n",
        "# ||       Section 1: Importing modules    ||\n",
        "# ||                                       ||\n",
        "# ===========================================\n",
        "\n",
        "from fastai.text.all import *\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import optuna\n",
        "\n",
        "# ===========================================\n",
        "# ||                                       ||\n",
        "# ||       Section 2: getting dataframes   ||\n",
        "# ||                    and dataloader     ||\n",
        "# ||                                       ||\n",
        "# ===========================================\n",
        "\n",
        "#TODO use clean dataset\n",
        "# Read in a CSV files\n",
        "\n",
        "train_df = pd.read_csv(\"/content/drive/MyDrive/ML_proj/zaazazza/augmented_cleaned_train_df.csv\")\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/ML_proj/zaazazza/clean_test_data.csv\")\n",
        "validation_df =  pd.read_csv(\"/content/drive/MyDrive/ML_proj/zaazazza/clean_validation_data.csv\")\n",
        "\n",
        "# Drop not needed columns\n",
        "\n",
        "train_df = train_df.drop(train_df.columns[0:5], axis=1)\n",
        "validation_df = validation_df.drop(validation_df.columns[0:5], axis=1)\n",
        "test_df = test_df.drop(test_df.columns[0:5], axis=1)\n",
        "\n",
        "# Create a data loader for text data using the \"TextDataLoaders\" class from the fastai library.\n",
        "dls = TextDataLoaders.from_df(train_df, valid_df=validation_df, path='.', valid_pct=0.2, seed=None,\n",
        "                              text_col=0, label_col=1, label_delim=None,\n",
        "                              y_block=None, text_vocab=None, is_lm=False,\n",
        "                              valid_col=None, tok_tfm=None,\n",
        "                              tok_text_col='text', seq_len=72)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# ||                                       ||\n",
        "# ||       Section 3: hyperparameter       ||\n",
        "# ||                        tuning         ||\n",
        "# ||                                       ||\n",
        "# ===========================================\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    # Define the search space for hyperparameters\n",
        "    dropout = trial.suggest_uniform('dropout', 0.2, 0.8)\n",
        "    n_epochs = trial.suggest_int('n_epochs', 2, 10)\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
        "\n",
        "    # Create a text classification learner with the suggested hyperparameters\n",
        "    learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=dropout, metrics=accuracy)\n",
        "\n",
        "    # Fine-tune the neural network for the suggested number of epochs using stochastic gradient descent\n",
        "    learn.fine_tune(n_epochs, learning_rate, cbs=[ShowGraphCallback()])\n",
        "\n",
        "    # Get the predicted probabilities for the validation data using the trained model.\n",
        "    val_dl = dls.test_dl(validation_df['text'])\n",
        "    val_preds, _ = learn.get_preds(dl=val_dl)\n",
        "\n",
        "    # Get the predicted labels for the validation data.\n",
        "    val_predicted_labels = val_preds.argmax(dim=1)\n",
        "\n",
        "    # Compute the f1 score of the model on the validation data using the `f1_score` function.\n",
        "    val_f1 = f1_score(validation_df[\"target\"].values, val_predicted_labels)\n",
        "\n",
        "    # Return the f1 score as the value to optimize\n",
        "    return val_f1\n",
        "\n",
        "# Create an optuna study and optimize the objective function \n",
        "#(we are maximizing the f1 socore)\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
        "study.optimize(objective, n_trials=30)\n",
        "\n",
        "# Print the best set of hyperparameters found by optuna and the corresponding f1 score on the validation data.\n",
        "print('Best trial:')\n",
        "best_trial = study.best_trial\n",
        "print(f'  Value: {best_trial.value:.5f}')\n",
        "print('  Params: ')\n",
        "for key, value in best_trial.params.items():\n",
        "    print(f'    {key}: {value}')\n",
        "\n",
        "# Train the model with the best hyperparameters found by optuna and evaluate it on the test data.\n",
        "best_dropout = best_trial.params['dropout']\n",
        "best_n_epochs = best_trial.params['n_epochs']\n",
        "best_learning_rate = best_trial.params['learning_rate']\n"
      ],
      "metadata": {
        "id": "KLKt3auyspaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# ||                                       ||\n",
        "# ||       Section 4: train the model      ||\n",
        "# ||                                       ||\n",
        "# ===========================================\n",
        "\n",
        "# Create a text classification learner using the fastai library.\n",
        "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=best_dropout, metrics=accuracy)\n",
        "\n",
        "# Fine-tune the neural network for four epochs using stochastic gradient descent\n",
        "learn.fine_tune(best_n_epochs, best_learning_rate, cbs=[ShowGraphCallback()])\n"
      ],
      "metadata": {
        "id": "zicLpN0rsu03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# ||                                       ||\n",
        "# ||       Section 5: testing the model    ||\n",
        "# ||                                       ||\n",
        "# ===========================================\n",
        "\n",
        "# Create a test dataloader from the test data using the `test_dl` method of the `dls` dataloaders object.\n",
        "test_dl = dls.test_dl(test_df['text'])\n",
        "\n",
        "# Get the predicted probabilities for the test data using the trained model.\n",
        "preds, _ = learn.get_preds(dl=test_dl)\n",
        "\n",
        "# Get the predicted labels for the test data.\n",
        "predicted_labels = preds.argmax(dim=1)\n",
        "\n",
        "# Convert the predicted labels to Python list and get the corresponding class names.\n",
        "predicted_classes = [dls.vocab[i] for i in predicted_labels]\n",
        "\n",
        "# Convert the predicted classes list to a tensor.\n",
        "predicted_classes_tensor = torch.tensor(predicted_labels)\n",
        "\n",
        "# Reshape the predicted tensor to have the same shape as the target tensor.\n",
        "predicted_classes_tensor = predicted_classes_tensor.unsqueeze(1)\n",
        "\n",
        "# Convert the target labels to a tensor.\n",
        "target_tensor = torch.tensor(test_df[\"target\"].values)\n",
        "\n",
        "# Compute the accuracy and f1 score of the model on the test data using the `accuracy` and `f1_score` functions.\n",
        "acc = accuracy(predicted_classes_tensor, target_tensor)\n",
        "f1 = f1_score(target_tensor, predicted_classes_tensor)\n",
        "\n",
        "# Print the accuracy and f1 score of the model on the test data.\n",
        "print(f\"Test accuracy: {acc}\")\n",
        "print(f\"Test f1 score: {f1}\")"
      ],
      "metadata": {
        "id": "dqE0JsGwsysy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.export(\"AWD_LSTM_aug.pkl\")\n",
        "from google.colab import files\n",
        "files.download('AWD_LSTM_aug.pkl')"
      ],
      "metadata": {
        "id": "PaxLkkhaehwK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
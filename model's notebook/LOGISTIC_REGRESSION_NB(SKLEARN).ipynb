{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "90qdjv7qCB_4",
        "kidR3CkQEAVz"
      ],
      "mount_file_id": "1MeqTa67L2M4RW75CGBkcKC0yaJKwviWk",
      "authorship_tag": "ABX9TyMo9VEim2sI7ZWZW1X5gx0u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tommasomncttn/NLP-Disaster-Tweet-Detection/blob/main/model's%20notebook/LOGISTIC_REGRESSION_NB(SKLEARN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn-genetic-opt"
      ],
      "metadata": {
        "id": "DjNBnXXj3RLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCvwKFmtByC7"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "from sklearn_genetic import GASearchCV\n",
        "from sklearn_genetic.space import Continuous, Categorical, Integer\n",
        "from sklearn_genetic.plots import plot_fitness_evolution, plot_search_space\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing files into DataFrames\n",
        "df_train = pd.read_csv(\"/content/drive/MyDrive/ML_proj/zaazazza/Copia de train_df.csv\")\n",
        "df_test = pd.read_csv(\"/content/drive/MyDrive/ML_proj/zaazazza/Copia de test_df.csv\")\n",
        "df_val = pd.read_csv(\"/content/drive/MyDrive/ML_proj/zaazazza/Copia de validation_df.csv\")"
      ],
      "metadata": {
        "id": "cHG2uK3o3Woo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the text and target labels from the training and testing data\n",
        "train_text = df_train[\"text\"]\n",
        "train_lable = df_train[\"target\"]\n",
        "\n",
        "test_text = df_test[\"text\"]\n",
        "test_lable = df_test[\"target\"]\n",
        "\n",
        "df_val.drop(columns = [\"Unnamed: 0\", \"id\", \"keyword\", \"location\"])\n",
        "\n",
        "\n",
        "# Creating a CountVectorizer object to convert the text into a matrix of token counts\n",
        "vect = CountVectorizer(stop_words='english')\n",
        "\n",
        "# Fitting the CountVectorizer on the training data to learn the vocabulary and create a document-term matrix\n",
        "vectorized_train = vect.fit_transform(train_text)\n",
        "\n",
        "# Creating a logistic regression model\n",
        "model = LogisticRegression(max_iter = 500, solver = \"lbfgs\")"
      ],
      "metadata": {
        "id": "EQY4nsfWD5BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the hyperparameter grid\n",
        "param_grid = {'C': Continuous(0.01, 10000), 'penalty': Categorical(['l2', None])}\n",
        "\n",
        "# Performing the hyperparameter search using GASearchCV\n",
        "model_H = GASearchCV(model,  cv = 3, param_grid = param_grid,  scoring=\"accuracy\", population_size=20, generations = 50, verbose = True) #TODO: change numbers of pop and gen (20 and 50 ish)\n",
        "model_H.fit(vectorized_train, train_lable)"
      ],
      "metadata": {
        "id": "pjJSfPgF4TRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Show best values found and plot the fitness graph\n",
        "print(\"Best parameters\", model_H.best_params_)\n",
        "plot_fitness_evolution(model_H)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PWymlxB43gf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing the test data using the same vocabulary as the training data\n",
        "vectorized_test = vect.transform(test_text)\n",
        "\n",
        "# Making predictions on the test data using the trained model\n",
        "predicted_lables = model_H.predict(vectorized_test)\n",
        "\n",
        "# Evaluating the performance of the model using accuracy score and confusion matrix\n",
        "accuracy = accuracy_score(test_lable, predicted_lables)\n",
        "confusion = confusion_matrix(test_lable, predicted_lables)\n",
        "f1 = f1_score(test_lable, predicted_lables)\n",
        "\n",
        "# Printing the accuracy and confusion matrix of the model\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Confusion Matrix:\\n\", confusion)\n",
        "print(\"F1 score:\", f1)"
      ],
      "metadata": {
        "id": "0usIJcK13l0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model to a file\n",
        "filename = 'LogistiRegression.sav'\n",
        "pickle.dump(model_H, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "w3hEYLMW3ojB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of folds to use for cross-validation\n",
        "num_folds = 5\n",
        "\n",
        "# Create a KFold object to split the data into K folds\n",
        "kf = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# Create an empty list to store F1 scores for each fold\n",
        "f1_scores = []\n",
        "\n",
        "# Loop over each fold and train the model on the training data, then evaluate on the validation data\n",
        "for fold, (train_indices, val_indices) in enumerate(kf.split(vectorized_train, train_lable)):\n",
        "\n",
        "    # Split the data into training and validation sets for this fold\n",
        "    X_train, y_train = vectorized_train[train_indices], train_lable[train_indices]\n",
        "    X_val, y_val = vectorized_train[val_indices], train_lable[val_indices]\n",
        "\n",
        "    # Fit a logistic regression model on the training data\n",
        "    model_H.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the validation data using the trained model\n",
        "    predicted_labels = model.predict(X_val)\n",
        "\n",
        "    # Evaluate the performance of the model using F1 score\n",
        "    f1 = f1_score(predicted_labels, y_val) \n",
        "\n",
        "    # Add the F1 score for this fold to the list\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "    # Print the F1 score for this fold\n",
        "    print(f\"Fold {fold}: F1 score = {f1}\")\n",
        "\n",
        "# Calculate the average F1 score across all folds\n",
        "avg_score = np.mean(f1_scores)\n",
        "print(f\"Average F1 score across {num_folds} folds: {avg_score}\")"
      ],
      "metadata": {
        "id": "fcqYV39M3o_y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
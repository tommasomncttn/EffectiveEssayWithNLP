{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1LY1-K0QIID2RkjVsf27PIAOoEvXAR2Ox",
      "authorship_tag": "ABX9TyN3q/WYFDj54dEYbOyQ50AV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tommasomncttn/NLP-Disaster-Tweet-Detection/blob/main/model's%20notebook/ULMfiT_NB(FASTAI).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFuuZDpsBUku"
      },
      "outputs": [],
      "source": [
        "# ===========================================\n",
        "# ||                                       ||\n",
        "# ||       Importing modules               ||\n",
        "# ||                                       ||\n",
        "# ===========================================\n",
        "\n",
        "from fastai.text.all import *\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import optuna\n",
        "\n",
        "                                  # ===========================================\n",
        "                                  # ||                                       ||\n",
        "                                  # ||       PART 1: Language Model          ||\n",
        "                                  # ||                                       ||\n",
        "                                  # ===========================================\n",
        "# ===========================================\n",
        "# ||                                       ||\n",
        "# ||       Section 1: getting dataframes   ||\n",
        "# ||                    and dataloader     ||\n",
        "# ||                                       ||\n",
        "# ===========================================\n",
        "\n",
        "\n",
        "# Read the CSV file into a variable\n",
        "dataset = pd.read_csv(\"/content/drive/MyDrive/ML_proj/tweets.csv\",)\n",
        "\n",
        "# Drop the \"id\", \"keyword\", \"location\", and \"target\" columns from the dataset\n",
        "dataset.drop(columns = [\"id\", \"keyword\", \"location\", \"target\"], inplace =True)\n",
        "\n",
        "# Create a dataloader for language modeling\n",
        "dls_lm = TextDataLoaders.from_df(dataset, path='.', valid_pct=0.2, seed=None,\n",
        "                          text_col=0, label_col=1, label_delim=None,\n",
        "                          y_block=None, text_vocab=None, is_lm=True,\n",
        "                          valid_col=None, tok_tfm=None,\n",
        "                          tok_text_col='text', seq_len=72, bs=32)\n",
        "\n",
        "\n",
        "# ===========================================\n",
        "# ||                                       ||\n",
        "# ||       Section 2: hyperparameter       ||\n",
        "# ||                        tuning         ||\n",
        "# ||                                       ||\n",
        "# ===========================================\n",
        "\n",
        "\n",
        "def train_language_model(trial):\n",
        "\n",
        "    print(\" NUOVO ROUND LM HYP. SEARCH\")\n",
        "    # Define the hyperparameters to optimize\n",
        "    wheight_decay_HP1 = trial.suggest_float('wheight_decay_HP1', 1e-5, 1e-1, log=True)\n",
        "    learningrate_1_HP2 = trial.suggest_float('learningrate_1_HP2', 1e-5, 1e-1, log=True)\n",
        "    learningrate_2_HP3 = trial.suggest_float('learningrate_2_HP3', 1e-5, 1e-1, log=True)\n",
        "    epoch1_HP4 = trial.suggest_int('epoch1_HP4', 1, 10)\n",
        "    epoch2_HP5 = trial.suggest_int('epoch2_HP5', 1, 10)\n",
        "\n",
        "    # Create a learner object\n",
        "    learn = language_model_learner(dls_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], wd=wheight_decay_HP1).to_fp16()\n",
        "\n",
        "    # Fine-tune the language model for one epoch\n",
        "    learn.fit_one_cycle(n_epoch=epoch1_HP4, lr_max=learningrate_1_HP2)\n",
        "\n",
        "    # Unfreeze the layers of the language model and fine-tune it\n",
        "    learn.unfreeze()\n",
        "    learn.fit_one_cycle(n_epoch=epoch2_HP5, lr_max=learningrate_2_HP3)\n",
        "\n",
        "    # Get the validation loss after fine-tuning\n",
        "    val_loss = learn.validate()[0]\n",
        "\n",
        "    return val_loss\n",
        "\n",
        "# Run the hyperparameter optimization for the language model\n",
        "study_lm = optuna.create_study(direction='minimize')\n",
        "study_lm.optimize(train_language_model, n_trials=1)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_wheight_decay_HP1 = study_lm.best_params['wheight_decay_HP1']\n",
        "best_learningrate_1_HP2 = study_lm.best_params['learningrate_1_HP2']\n",
        "best_learningrate_2_HP3 = study_lm.best_params['learningrate_2_HP3']\n",
        "best_epoch_1_HP4 = study_lm.best_params['epoch1_HP4']\n",
        "best_epoch_2_HP5 = study_lm.best_params['epoch2_HP5']\n",
        "\n",
        "\n",
        "# ===========================================\n",
        "# ||                                       ||\n",
        "# ||       Section 3: train the language   ||\n",
        "# ||                            model      ||\n",
        "# ||                                       ||\n",
        "# ===========================================\n",
        "\n",
        "\n",
        "# Create a learner object\n",
        "learn = language_model_learner(dls_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], wd=best_wheight_decay_HP1).to_fp16()\n",
        "\n",
        "# Fine-tune the language model for one epoch\n",
        "learn.fit_one_cycle(n_epoch = best_epoch_1_HP4, lr_max= best_learningrate_1_HP2)\n",
        "\n",
        "# Unfreeze the layers of the language model and fine-tune it\n",
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(n_epoch = best_epoch_2_HP5, lr_max= best_learningrate_2_HP3)\n",
        "\n",
        "# Save the encoder part of the fine-tuned language model\n",
        "learn.save_encoder('finetuned')\n",
        "\n",
        "\n",
        "                                  # ===========================================\n",
        "                                  # ||                                       ||\n",
        "                                  # ||       PART 2: Classifier              ||\n",
        "                                  # ||                                       ||\n",
        "                                  # ===========================================\n",
        "# ===========================================\n",
        "# ||                                       ||\n",
        "# ||       Section 1: getting dataframes   ||\n",
        "# ||                    and dataloader     ||\n",
        "# ||                                       ||\n",
        "# ===========================================\n",
        "\n",
        "\n",
        "#TODO use clean dataset\n",
        "# Read in a CSV file\n",
        "train_df = pd.read_csv(\"/content/drive/MyDrive/ML_proj/zaazazza/train_df.csv\")\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/ML_proj/zaazazza/test_df.csv\")\n",
        "validation_df =  pd.read_csv(\"/content/drive/MyDrive/ML_proj/zaazazza/validation_df.csv\")\n",
        "\n",
        "# Drop not needed columns\n",
        "train_df = test_df.drop(train_df.columns[0:4], axis=1)\n",
        "validation_df = validation_df.drop(validation_df.columns[0:4], axis=1)\n",
        "test_df = test_df.drop(test_df.columns[0:4], axis=1)\n",
        "\n",
        "# Create a dataloader\n",
        "dls_clas = TextDataLoaders.from_df(train_df, valid_df=validation_df, path='.', valid_pct=0.2, seed=None,\n",
        "                          text_col=0, label_col=1, label_delim=None,\n",
        "                          y_block=None, text_vocab=dls_lm.vocab, is_lm=False,\n",
        "                          valid_col=None, tok_tfm=None,\n",
        "                          tok_text_col='text', seq_len=72)\n",
        "\n",
        "\n",
        "# ===========================================\n",
        "# ||                                       ||\n",
        "# ||       Section 2: hyperparameter       ||\n",
        "# ||                        tuning         ||\n",
        "# ||                                       ||\n",
        "# ===========================================\n",
        "\n",
        "\n",
        "def train_classifier_model(trial):\n",
        "\n",
        "    print(\" NUOVO ROUND class HYP. SEARCH\")\n",
        "\n",
        "    # Define the hyperparameters as Optuna parameters\n",
        "    lr_1_hp5 = trial.suggest_loguniform('lr_1_hp5', 1e-4, 1e-1)\n",
        "    lr_2_hp6 = trial.suggest_categorical('lr_2_hp6', [slice(1e-2/(2.6**4),1e-2), slice(1e-3/(2.6**4),1e-3)])\n",
        "    lr_3_hp7 = trial.suggest_categorical('lr_3_hp7', [slice(5e-3/(2.6**4),5e-3), slice(1e-3/(2.6**4),1e-3)])\n",
        "    lr_4_hp8 = trial.suggest_loguniform('lr_4_hp8', 1e-7, 1e-3)\n",
        "    drop_mult_hp9 = trial.suggest_uniform('drop_mult_hp9', 0.1, 0.9)\n",
        "\n",
        "    # Create a learner object for training a text classifier\n",
        "    learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=drop_mult_hp9, metrics=accuracy)\n",
        "\n",
        "    # Load the encoder part of the previously fine-tuned language model to the classifier learner object\n",
        "    learn = learn.load_encoder('finetuned')\n",
        "\n",
        "    # Train the classifier for one epoch\n",
        "    learn.fit_one_cycle(1, lr_1_hp5)\n",
        "\n",
        "    # Freeze all but the last two layers of the classifier and fine-tune it for one epoch\n",
        "    learn.freeze_to(-2)\n",
        "    learn.fit_one_cycle(1, lr_2_hp6)\n",
        "\n",
        "    # Freeze all but the last three layers of the classifier and fine-tune it for one epoch\n",
        "    learn.freeze_to(-3)\n",
        "    learn.fit_one_cycle(1, lr_3_hp7)\n",
        "\n",
        "    # Unfreeze all layers of the classifier and fine-tune it for three epochs\n",
        "    learn.unfreeze()\n",
        "    learn.fine_tune(2, lr_4_hp8, cbs=[ShowGraphCallback()])\n",
        "\n",
        "        # Get the predicted probabilities for the validation data using the trained model.\n",
        "    val_dl = dls_clas.test_dl(validation_df['text'])\n",
        "    val_preds, _ = learn.get_preds(dl=val_dl)\n",
        "\n",
        "    # Get the predicted labels for the validation data.\n",
        "    val_predicted_labels = val_preds.argmax(dim=1)\n",
        "\n",
        "    # Compute the f1 score of the model on the validation data using the `f1_score` function.\n",
        "    val_f1 = f1_score(validation_df[\"target\"].values, val_predicted_labels)\n",
        "\n",
        "    # Return the negative f1 score as the loss to optimize (because optuna maximizes the negative of the objective).\n",
        "    return -val_f1\n",
        "\n",
        "# Create an optuna study and optimize the objective function using the TPE sampler.\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
        "study.optimize(train_classifier_model, n_trials=2)\n",
        "\n",
        "# Print the best set of hyperparameters found by optuna and the corresponding f1 score on the validation data.\n",
        "print('Best trial:')\n",
        "best_trial = study.best_trial\n",
        "print(f'  Value: {-best_trial.value:.5f}')\n",
        "print('  Params: ')\n",
        "for key, value in best_trial.params.items():\n",
        "    print(f'    {key}: {value}')\n",
        "\n",
        "# Hyperparameters\n",
        "\n",
        "best_lr_1_hp5 = best_trial.params['lr_1_hp5']\n",
        "best_lr_2_hp6 = best_trial.params['lr_2_hp6']\n",
        "best_lr_3_hp7 = best_trial.params['lr_3_hp7']\n",
        "best_lr_4_hp8 = best_trial.params['lr_4_hp8']\n",
        "best_drop_mult_hp9 = best_trial.params['drop_mult_hp9']\n",
        "\n",
        "\n",
        "# ===========================================\n",
        "# ||                                       ||\n",
        "# ||       Section 3: train the model      ||\n",
        "# ||                                       ||\n",
        "# ===========================================\n",
        "\n",
        "\n",
        "# Create a learner object for training a text classifier\n",
        "learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=best_drop_mult_hp9, metrics=accuracy)\n",
        "\n",
        "# Load the encoder part of the previously fine-tuned language model to the classifier learner object\n",
        "learn = learn.load_encoder('finetuned')\n",
        "\n",
        "\n",
        "\n",
        "# Train the classifier for one epoch\n",
        "learn.fit_one_cycle(1, best_lr_1_hp5)\n",
        "\n",
        "# Freeze all but the last two layers of the classifier and fine-tune it for one epoch\n",
        "learn.freeze_to(-2)\n",
        "learn.fit_one_cycle(1, best_lr_2_hp6)\n",
        "\n",
        "# Freeze all but the last three layers of the classifier and fine-tune it for one epoch\n",
        "learn.freeze_to(-3)\n",
        "learn.fit_one_cycle(1, best_lr_3_hp7)\n",
        "\n",
        "# Unfreeze all layers of the classifier and fine-tune it for three epochs\n",
        "learn.unfreeze()\n",
        "learn.fine_tune(2, best_lr_4_hp8, cbs=[ShowGraphCallback()])\n",
        "\n",
        "# ===========================================\n",
        "# ||                                       ||\n",
        "# ||       Section 4: test the model       ||\n",
        "# ||                                       ||\n",
        "# ===========================================\n",
        "\n",
        "# Create a test dataloader from the test data using the `test_dl` method of the `dls` dataloaders object.\n",
        "test_dl = dls_clas.test_dl(test_df['text'])\n",
        "\n",
        "# Get the predicted probabilities for the test data using the trained model.\n",
        "preds, _ = learn.get_preds(dl=test_dl)\n",
        "\n",
        "# Get the predicted labels for the test data.\n",
        "predicted_labels = preds.argmax(dim=1)\n",
        "\n",
        "# Convert the predicted labels to Python list and get the corresponding class names.\n",
        "predicted_classes = [dls_clas.vocab[i] for i in predicted_labels]\n",
        "\n",
        "# Convert the predicted classes list to a tensor.\n",
        "predicted_classes_tensor = torch.tensor(predicted_labels)\n",
        "\n",
        "# Reshape the predicted tensor to have the same shape as the target tensor.\n",
        "predicted_classes_tensor = predicted_classes_tensor.unsqueeze(1)\n",
        "\n",
        "# Convert the target labels to a tensor.\n",
        "target_tensor = torch.tensor(test_df[\"target\"].values)\n",
        "\n",
        "# Compute the accuracy and f1 score of the model on the test data using the `accuracy` and `f1_score` functions.\n",
        "acc = accuracy(predicted_classes_tensor, target_tensor)\n",
        "f1 = f1_score(target_tensor, predicted_classes_tensor)\n",
        "\n",
        "# Print the accuracy and f1 score of the model on the test data.\n",
        "print(f\"Test accuracy: {acc}\")\n",
        "print(f\"Test f1 score: {f1}\")"
      ]
    }
  ]
}
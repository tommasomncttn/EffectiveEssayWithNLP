{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install pynvml\n",
        "!pip install torch\n",
        "!pip install optuna"
      ],
      "metadata": {
        "id": "POR6RRzlSaEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils functions for gpu and device  "
      ],
      "metadata": {
        "id": "dGZ4FIzbM-rU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo\n",
        "from torch import cuda\n",
        "import gc\n",
        "\n",
        "def check_gpu_availability():\n",
        "    # Check if CUDA is available\n",
        "    print(f\"Cuda is available: {torch.cuda.is_available()}\")\n",
        "\n",
        "\n",
        "def getting_device(gpu_prefence=True) -> torch.device:\n",
        "    \"\"\"\n",
        "    This function gets the torch device to be used for computations,\n",
        "    based on the GPU preference specified by the user.\n",
        "    \"\"\"\n",
        "\n",
        "    # If GPU is preferred and available, set device to CUDA\n",
        "    if gpu_prefence and torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "    # If GPU is not preferred or not available, set device to CPU\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    # Print the selected device\n",
        "    print(f\"Selected device: {device}\")\n",
        "\n",
        "    # Return the device\n",
        "    return device\n",
        "\n",
        "\n",
        "# Define a function to print GPU memory utilization\n",
        "def print_gpu_utilization():\n",
        "    # Initialize the PyNVML library\n",
        "    nvmlInit()\n",
        "    # Get a handle to the first GPU in the system\n",
        "    handle = nvmlDeviceGetHandleByIndex(0)\n",
        "    # Get information about the memory usage on the GPU\n",
        "    info = nvmlDeviceGetMemoryInfo(handle)\n",
        "    # Print the GPU memory usage in MB\n",
        "    print(f\"GPU memory occupied: {info.used // 1024 ** 2} MB.\")\n",
        "\n",
        "\n",
        "# Define a function to print training summary information\n",
        "def print_summary(result):\n",
        "    # Print the total training time in seconds\n",
        "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
        "    # Print the number of training samples processed per second\n",
        "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
        "    # Print the GPU memory utilization\n",
        "    print_gpu_utilization()\n",
        "\n",
        "def clean_gpu():\n",
        "    # Get current GPU memory usage\n",
        "    print(\"BEFORE CLEANING:\")\n",
        "    print(f\"Allocated: {cuda.memory_allocated() / 1024 ** 3:.2f} GB\")\n",
        "    print(f\"Cached: {cuda.memory_cached() / 1024 ** 3:.2f} GB\")\n",
        "    print(\"\\n\")\n",
        "    # Free up PyTorch and CUDA memory\n",
        "    torch.cuda.empty_cache()\n",
        "    cuda.empty_cache()\n",
        "\n",
        "    # Run garbage collection to free up other memory\n",
        "    gc.collect()\n",
        "\n",
        "    # Get new GPU memory usage\n",
        "    print(\"AFTER CLEANING:\")\n",
        "    print(f\"Allocated: {cuda.memory_allocated() / 1024 ** 3:.2f} GB\")\n",
        "    print(f\"Cached: {cuda.memory_cached() / 1024 ** 3:.2f} GB\")"
      ],
      "metadata": {
        "id": "CVNLnZuDM6dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Importing modules "
      ],
      "metadata": {
        "id": "vf8oRuXaNVmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "import torch.nn as nn\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "import torchvision.models as models\n",
        "import optuna\n",
        "#from utils4gpu import *\n",
        "\n",
        "clean_gpu()\n"
      ],
      "metadata": {
        "id": "Dehjo1-VOBIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: Utlis for Bert "
      ],
      "metadata": {
        "id": "g7nAwG_5ONCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# FUNCTION 4 MAX LENGTH TO SET TOKENIZER MAX LENGTH\n",
        "\n",
        "def get_list_of_lengths(text_column, tokenizer) -> int:\n",
        "    token_lens = []\n",
        "\n",
        "    for text in text_column:\n",
        "        # Tokenize the text and add `[CLS]` and `[SEP]` tokens => split in symbolic/textual tokens and map them to integer ids\n",
        "        tokens = tokenizer.encode(text, add_special_tokens=True)\n",
        "\n",
        "        # checking the len of tokenized sentence\n",
        "        token_lens.append(len(tokens))\n",
        "\n",
        "    return token_lens\n",
        "\n",
        "\n",
        "def get_max_lenghts(list_len) -> int:\n",
        "    # PART 1 MAX\n",
        "\n",
        "    # Convert the list to a PyTorch tensor\n",
        "    tensor_data = torch.tensor(list_len)\n",
        "\n",
        "    # getting the argmax index\n",
        "    argmax_index = tensor_data.argmax().item()\n",
        "\n",
        "    # getting the argmax\n",
        "\n",
        "    argmax = list_len[argmax_index]\n",
        "    print(f\"The longest input sequence has value: {argmax}\")\n",
        "\n",
        "    # PART 2 HISTOGRAM\n",
        "\n",
        "    # importing the library for the visualization\n",
        "    import seaborn as sns\n",
        "\n",
        "    # now we want to plot the histogram of the list of integers\n",
        "    sns.histplot(list_len, bins=10)\n",
        "\n",
        "    return argmax\n",
        "\n",
        "\n",
        "# FUNCTION 4 GETTING THE DATASET IN  A FORM 4 PYTORCH\n",
        "\n",
        "def convert_to_torch(item):\n",
        "    input_ids = item['input_ids']\n",
        "    token_type_ids = item['token_type_ids']\n",
        "    attention_mask = item['attention_mask']\n",
        "    label = item['labels']\n",
        "    return {'input_ids': input_ids, 'token_type_ids': token_type_ids, 'attention_mask': attention_mask, 'label': label}\n"
      ],
      "metadata": {
        "id": "RjxVVZs7OU8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3: Checking gpu, choosing device, and model"
      ],
      "metadata": {
        "id": "_RIaYEFmOWuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CHECK IF GPU IS UP\n",
        "check_gpu_availability()\n",
        "\n",
        "# SAVE THE DEVICE WE ARE WORKING WITH\n",
        "device = getting_device(gpu_prefence=True)\n",
        "\n",
        "# SHOULD BE FEW MB\n",
        "print_gpu_utilization()\n",
        "\n",
        "# SETTING HF CHECKPOINT/MODEL\n",
        "model_nm = \"bert-large-uncased\""
      ],
      "metadata": {
        "id": "s4NJaVxOOlKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4: Importing doc and split"
      ],
      "metadata": {
        "id": "oIXvg72SOmj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read csv files to create pandas dataframes\n",
        "path2test = '/content/drive/MyDrive/ML_proj/zaazazza/Copia de test_df.csv'\n",
        "test_df = pd.read_csv(path2test)\n",
        "\n",
        "path2val = '/content/drive/MyDrive/ML_proj/zaazazza/Copia de validation_df.csv'\n",
        "validation_df = pd.read_csv(path2val)\n",
        "\n",
        "path2train = '/content/drive/MyDrive/ML_proj/zaazazza/Copia de train_df.csv'\n",
        "train_df = pd.read_csv(path2train)\n",
        "\n",
        "# pandas2dataset\n",
        "train_df = train_df.rename(columns={'target': 'labels'})\n",
        "validation_df = validation_df.rename(columns={'target': 'labels'})\n",
        "test_df = test_df.rename(columns={'target': 'labels'})\n",
        "\n",
        "ds_train = Dataset.from_pandas(train_df[[\"text\", \"labels\"]])\n",
        "ds_validation = Dataset.from_pandas(validation_df[[\"text\", \"labels\"]])\n",
        "ds_test = Dataset.from_pandas(test_df[[\"text\", \"labels\"]])"
      ],
      "metadata": {
        "id": "lihgKM0WQ0Mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 5: Tokenization, Tensorization and Collider"
      ],
      "metadata": {
        "id": "vf9n2qadQ61B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# IMPORTING OUR TOKENIZER\n",
        "tokz = AutoTokenizer.from_pretrained(model_nm)\n",
        "\n",
        "# GETTING MAX LENGTH\n",
        "max_length = get_max_lenghts(get_list_of_lengths(ds_train[\"text\"], tokz))\n",
        "\n",
        "# DEFINING A TOKENIZE FUNCTION TO TOKENIZE BOTH THE TWO DATASETS\n",
        "def tok_func(x): return tokz(x[\"text\"], truncation=True, padding = \"max_length\", max_length=max_length)\n",
        "\n",
        "# CHECK THAT TOKENIZER FUNCTION WORKS\n",
        "tok_func(ds_train[19]) # the 1 are for padding it; the attention mask show to not care about the 1\n",
        "\n",
        "# TOKENIZING THE DS\n",
        "tok_ds_train = ds_train.map(tok_func, batched=True, remove_columns=['text'])\n",
        "tok_ds_validation = ds_validation.map(tok_func, batched=True, remove_columns=['text'])\n",
        "tok_ds_test = ds_test.map(tok_func, batched=True, remove_columns=['text'])\n",
        "\n",
        "# Use the `map()` method to apply the `convert_to_torch()` function to each item in the dataset\n",
        "tok_ds_train = tok_ds_train.map(convert_to_torch, batched=True)\n",
        "\n",
        "# Convert the dataset to a PyTorch TensorDataset by simply transforming each numerical column of the dataset in tensor\n",
        "tensor_train_dataset = torch.utils.data.TensorDataset(torch.tensor(tok_ds_train['input_ids']),\n",
        "                                                torch.tensor(tok_ds_train['token_type_ids']),\n",
        "                                                torch.tensor(tok_ds_train['attention_mask']),\n",
        "                                                torch.tensor(tok_ds_train['label']))\n",
        "# SAME FOR VALIDATION SET\n",
        "\n",
        "# Use the `map()` method to apply the `convert_to_torch()` function to each item in the dataset\n",
        "tok_ds_validation = tok_ds_validation.map(convert_to_torch, batched=True)\n",
        "\n",
        "# Convert the dataset to a PyTorch TensorDataset\n",
        "tensor_validation_dataset = torch.utils.data.TensorDataset(torch.tensor(tok_ds_validation['input_ids']),\n",
        "                                                torch.tensor(tok_ds_validation['token_type_ids']),\n",
        "                                                torch.tensor(tok_ds_validation['attention_mask']),\n",
        "                                                torch.tensor(tok_ds_validation['label']))\n",
        "\n",
        "# SAME FOR TEST SET\n",
        "\n",
        "# Use the `map()` method to apply the `convert_to_torch()` function to each item in the dataset\n",
        "tok_ds_test = tok_ds_test.map(convert_to_torch, batched=True)\n",
        "\n",
        "# Convert the dataset to a PyTorch TensorDataset\n",
        "tensor_test_dataset = torch.utils.data.TensorDataset(torch.tensor(tok_ds_test['input_ids']),\n",
        "                                                torch.tensor(tok_ds_test['token_type_ids']),\n",
        "                                                torch.tensor(tok_ds_test['attention_mask']),\n",
        "                                                torch.tensor(tok_ds_test['label']))\n",
        "\n",
        "\n",
        "# PLUGGING INTO DATALOADERS\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "            tensor_train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(tensor_train_dataset), # Select batches randomly\n",
        "            batch_size = 32 # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            tensor_validation_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(tensor_validation_dataset), # Select batches randomly\n",
        "            batch_size = 32 # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "            tensor_test_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(tensor_test_dataset), # Select batches randomly\n",
        "            batch_size = 32 # Trains with this batch size.\n",
        "        )\n"
      ],
      "metadata": {
        "id": "MDN1KFX6RDiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 6: Building the model"
      ],
      "metadata": {
        "id": "pXNHZlSQREi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# creating a pytorch module => that is a block of parameters and computation (forward)\n",
        "class Bert4BinaryClassification(nn.Module):\n",
        "\n",
        "    # initiliazer, specify the name of the bert model you want to load\n",
        "    def __init__(self, model_name):\n",
        "\n",
        "        # be sure the nn.Module is correctly\n",
        "        super().__init__()\n",
        "\n",
        "        # initialize the model (think to it as a cooler sequential(...))\n",
        "        self.model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(getting_device())\n",
        "\n",
        "    # forward method, we need to feed it with the tokenized text (ids + attention mask)\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "\n",
        "        # pass the tokenized test through the model, which has as last layer a FNN with 2 output perceptrons\n",
        "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # gather the 2 entries output vector\n",
        "        logits = output.logits\n",
        "\n",
        "        # return it\n",
        "        return logits\n",
        "\n",
        "    # implement the gpu util function as a method so it move directly the model on the gpu\n",
        "    def getting_device(self, gpu_prefence=True):\n",
        "        \"\"\"\n",
        "        This function gets the torch device to be used for computations,\n",
        "        based on the GPU preference specified by the user.\n",
        "        \"\"\"\n",
        "\n",
        "        # If GPU is preferred and available, set device to CUDA\n",
        "        if gpu_prefence and torch.cuda.is_available():\n",
        "            device = torch.device('cuda')\n",
        "        # If GPU is not preferred or not available, set device to CPU\n",
        "        else:\n",
        "            device = torch.device(\"cpu\")\n",
        "\n",
        "        # Print the selected device\n",
        "        print(f\"Selected device for BERTBINARYCLASSIFICATION: {device}\")\n",
        "\n",
        "        # Return the device\n",
        "        return device\n",
        "\n",
        "\n",
        "# INITLIAZING THE MODEL AND CHECKING IF ON GPU\n",
        "model = Bert4BinaryClassification(model_nm)\n",
        "# checking if the model is on the gpu\n",
        "print_gpu_utilization()\n"
      ],
      "metadata": {
        "id": "KOZSMJ83RPtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 7: Training, Testing, and validation functions"
      ],
      "metadata": {
        "id": "B8I5Qf4SRTb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1) TRAINING\n",
        "\n",
        "def train(model, train_dataloader, loss_fn, optimizer, scheduler):\n",
        "    \"\"\"\n",
        "    This function trains a given model on training data and validates on validation data\n",
        "    \"\"\"\n",
        "    # setting the model to training mode => important because it says to compute gradients 4 backwards pass, while computing the forward pass\n",
        "    model.train()\n",
        "\n",
        "    # wrapping tqdm around the dataloader to allow visualization\n",
        "    visual_train_dl = tqdm(train_dataloader)\n",
        "\n",
        "    # initiliaze it to compute the training loss after all the batches\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # initialize for printing near the bar\n",
        "    train_step = 0\n",
        "\n",
        "    # iterate over the batch of the data loader (after all iteration we made 1 epoch)\n",
        "    for batch in visual_train_dl:\n",
        "        # accessing batch (contains an input_ids, an attention mask, and a label)\n",
        "        batch_ids = batch[0].to(device)\n",
        "        batch_attention_mask = batch[2].to(device)\n",
        "        # squeeze remove dimension because labels should not have dimension, long transform in long integer as required by pytorch\n",
        "        batch_labels = batch[3].squeeze().to(device).long()\n",
        "\n",
        "        # step 1: reset optimizer stored gradient\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # step 2: model logists through forward pass => remember the forward return logists\n",
        "        logits = model(batch_ids, batch_attention_mask).to(device)\n",
        "\n",
        "        # step 3: take the argmax index aka the predicted class index\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "        # detached_predictions = predictions.detach().cpu().numpy()\n",
        "\n",
        "        # step 4: compute the loss => takes as input the logist aka the predicted probability, not the predicted class\n",
        "        loss = loss_fn(logits, batch_labels)\n",
        "        # needed for printing stats\n",
        "        total_train_loss += loss.item()\n",
        "        train_step += 1\n",
        "\n",
        "        # step 5: compute the gradient (derivative of the loss over every trainable parameter)\n",
        "        loss.backward()\n",
        "\n",
        "        # step 6: advance the optimizer and the scheduler\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # The set_postfix() method of the progress bar updates the message displayed in the progress bar to include the specified key-value pairs\n",
        "        visual_train_dl.set_postfix({'train_loss': total_train_loss / train_step})\n",
        "    visual_train_dl.close()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "\n",
        "    final_avg_train_loss = total_train_loss / train_step\n",
        "\n",
        "    print('')\n",
        "    print('  Average training loss: {0:.2f}'.format(final_avg_train_loss))\n",
        "    return final_avg_train_loss\n",
        "\n",
        "\n",
        "# 2) VALIDATION WITH ACCURACY\n",
        "\n",
        "def validate(model, valid_dataloader, loss_fn):\n",
        "    # step 1, say to the model that computing the forward is enough, no backward!\n",
        "    with torch.no_grad():\n",
        "        # step 2, say to the model that it is validation time (4 dropout and normalization)\n",
        "        model.eval()\n",
        "\n",
        "        # initiliaze it to compute the training loss after all the batches\n",
        "        total_valid_loss = 0\n",
        "\n",
        "        # initiliaze to compute the avg thereafter\n",
        "        valid_step = 0\n",
        "\n",
        "        # list for accuracy\n",
        "        correct = 0\n",
        "\n",
        "        # wrapper for progress bar\n",
        "        visual_valid_dl = tqdm(valid_dataloader)\n",
        "\n",
        "        # iterate over the batch of the data loader (after all iteration we made 1 epoch)\n",
        "        for batch in visual_valid_dl:\n",
        "            # update the step\n",
        "            valid_step += 1\n",
        "\n",
        "            # accessing batch (contains an input_ids, an attention mask, and a label)\n",
        "            batch_ids = batch[0].to(device)\n",
        "            batch_attention_mask = batch[2].to(device)\n",
        "\n",
        "            # squeeze remove dimension because labels should not have dimension, long transform in long integer as required by pytorch\n",
        "            batch_labels = batch[3].squeeze().to(device).long()\n",
        "\n",
        "            # step 3: model logists through forward pass => remember the forward return logists\n",
        "            logits = model(batch_ids, batch_attention_mask).to(device)\n",
        "\n",
        "            # step 4: getting predictions\n",
        "            predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "            # step 5: check if correct\n",
        "            correct += (predictions == batch_labels).type(torch.float).sum().item()\n",
        "\n",
        "            # detached_predictions = predictions.detach().cpu().numpy()\n",
        "\n",
        "            # step 5: computing the loss\n",
        "            loss = loss_fn(logits, batch_labels)\n",
        "\n",
        "            # step 6: add to total loss\n",
        "            total_valid_loss += loss.item()\n",
        "\n",
        "        total_valid_loss /= valid_step\n",
        "        accuracy = correct / len(valid_dataloader.dataset)\n",
        "\n",
        "        print(f'Accuracy Score: {accuracy}')\n",
        "        print(f'Valid_loss: {total_valid_loss}')\n",
        "\n",
        "        return total_valid_loss\n",
        "\n",
        "\n",
        "# 3) TESTING WITH ACCURACY AND F1\n",
        "\n",
        "def test_with_f1(model, test_dataloader, loss_fn):\n",
        "    # step 1, say to the model that computing the forward is enough, no backward!\n",
        "    with torch.no_grad():\n",
        "        # step 2, say to the model that it is validation time (4 dropout and normalization)\n",
        "        model.eval()\n",
        "\n",
        "        # initiliaze it to compute the training loss after all the batches\n",
        "        total_test_loss = 0\n",
        "\n",
        "        # initiliaze to compute the avg thereafter\n",
        "        test_step = 0\n",
        "\n",
        "        # list for accuracy\n",
        "        correct = 0\n",
        "\n",
        "        # list for predictions and true labels\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "\n",
        "        # wrapper for progress bar\n",
        "        visual_test_dl = tqdm(test_dataloader)\n",
        "\n",
        "        # iterate over the batch of the data loader (after all iteration we made 1 epoch)\n",
        "        for batch in visual_test_dl:\n",
        "            # update the step\n",
        "            test_step += 1\n",
        "\n",
        "            # accessing batch (contains an input_ids, an attention mask, and a label)\n",
        "            batch_ids = batch[0].to(device)\n",
        "            batch_attention_mask = batch[2].to(device)\n",
        "\n",
        "            # squeeze remove dimension because labels should not have dimension, long transform in long integer as required by pytorch\n",
        "            batch_labels = batch[3].squeeze().to(device).long()\n",
        "\n",
        "            # step 3: model logists through forward pass => remember the forward return logists\n",
        "            logits = model(batch_ids, batch_attention_mask).to(device)\n",
        "\n",
        "            # step 4: getting predictions\n",
        "            predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "            # append predictions and labels to lists\n",
        "            all_predictions.extend(predictions.cpu().numpy().tolist())\n",
        "            all_labels.extend(batch_labels.cpu().numpy().tolist())\n",
        "\n",
        "            # step 5: check if correct\n",
        "            correct += (predictions == batch_labels).type(torch.float).sum().item()\n",
        "\n",
        "            # detached_predictions = predictions.detach().cpu().numpy()\n",
        "\n",
        "            # step 5: computing the loss\n",
        "            loss = loss_fn(logits, batch_labels)\n",
        "\n",
        "            # step 6: add to total loss\n",
        "            total_test_loss += loss.item()\n",
        "\n",
        "        total_test_loss /= test_step\n",
        "        accuracy = correct / len(test_dataloader.dataset)\n",
        "        f1 = f1_score(all_labels, all_predictions, average='macro')\n",
        "\n",
        "        print(f'Accuracy Score: {accuracy}')\n",
        "        print(f'F1 Score: {f1}')\n",
        "        print(f'Test_loss: {total_test_loss}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XsPCM8_1RtSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 8: loss, optimizer and hyperparameters"
      ],
      "metadata": {
        "id": "64q-d1EESIOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1) LOSS\n",
        "loss_fn = nn.CrossEntropyLoss() # input => (predicted probab positive class, positive or negative class)\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    # Set up hyperparameters\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-8, 1e-5)\n",
        "    epsilon_value = trial.suggest_loguniform('epsilon_value', 1e-10, 1e-6)\n",
        "    epochs_num = trial.suggest_int('epochs_num', 2,5)\n",
        "\n",
        "    total_steps = len(train_dataloader) * epochs_num \n",
        "    lower_ws = int(0.05 * total_steps)\n",
        "    high_ws = int(0.2 * total_steps)\n",
        "    warmup_st = trial.suggest_int('warmup_st', lower_ws,high_ws)\n",
        "\n",
        "    # setting up the optmizer and learner\n",
        "\n",
        "    # it is a transformer class, it requires two argument, a.k.a the parameters it needs to update at each step and the learning rate to scale the gradient\n",
        "    optimizer1 = AdamW(model.parameters(),\n",
        "                  lr = learning_rate, # args.learning_rate\n",
        "                  eps = epsilon_value # args.adam_epsilon\n",
        "                )\n",
        "    \n",
        "    # Define model and optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, eps=epsilon_value)\n",
        "\n",
        "    \n",
        "    # setting up the scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer1, num_warmup_steps = warmup_st,num_training_steps = total_steps)\n",
        "\n",
        "    # Train model and evaluate on validation set\n",
        "    for epoch in range(epochs_num):\n",
        "        # Training\n",
        "        loss_train = train(model = model, train_dataloader = train_dataloader, loss_fn = loss_fn, optimizer = optimizer1, scheduler = scheduler)\n",
        "\n",
        "    # Calculate validation loss and return as objective value\n",
        "    validation_loss = validate(model = model,valid_dataloader = validation_dataloader,loss_fn = loss_fn)\n",
        "    \n",
        "    return validation_loss\n",
        "\n",
        "# Create study object and run optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=50) #TODO INCREASE NUMBER OF TRIALS\n",
        "print(study.best_params)\n",
        "best_trial = study.best_trial\n",
        "\n",
        "# Train the model with the best hyperparameters found by optuna and evaluate it on the test data.\n",
        "best_learning_rate_hp1 = best_trial.params['learning_rate']\n",
        "best_epsilon_value_hp2 = best_trial.params['epsilon_value']\n",
        "best_epochs_hp3 = best_trial.params['epochs_num']\n",
        "best_warmup_steps_hp4 = best_trial.params['warmup_st']\n",
        "\n",
        "# setting up the optmizer\n",
        "\n",
        "# it is a transformer class, it requires two argument, a.k.a the parameters it needs to update at each step and the learning rate to scale the gradient\n",
        "\n",
        "optimizer1 = AdamW(model.parameters(),\n",
        "                  lr = best_learning_rate_hp1, # args.learning_rate\n",
        "                  eps = best_epsilon_value_hp2 # args.adam_epsilon\n",
        "                )\n",
        "\n",
        "# 3) LR SCHEDULER\n",
        "\n",
        "# hyperparameters 2: lr_scheduler\n",
        "\n",
        "total_steps = len(train_dataloader) * best_epochs_hp3 # Total number of training steps is the number of steps per epoch times the number of epochs\n",
        "\n",
        "# setting up the scheduler\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer1, num_warmup_steps = best_warmup_steps_hp4, num_training_steps = total_steps)\n"
      ],
      "metadata": {
        "id": "6N04dZnTSXWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 9: training and testing"
      ],
      "metadata": {
        "id": "YRgjmxUMSVTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initliazing history of loss\n",
        "\n",
        "train_loss_history = []\n",
        "validation_loss_history = []\n",
        "\n",
        "# TRAINING LOOP\n",
        "print(\" \")\n",
        "print(\"START TRAINING \")\n",
        "print(\" \")\n",
        "for t in range(best_epochs_hp3):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    loss_train = train(model = model, train_dataloader = train_dataloader, loss_fn = loss_fn, optimizer = optimizer1, scheduler = scheduler)\n",
        "    loss_validate = validate(model = model,valid_dataloader = validation_dataloader,loss_fn = loss_fn)\n",
        "    train_loss_history.append(loss_train)\n",
        "    validation_loss_history.append(loss_validate)\n",
        "print(\"DONE TRAINING\")\n",
        "\n",
        "# TESTING\n",
        "print(\" \")\n",
        "print(\"START TESTING\")\n",
        "print(\" \")\n",
        "test_with_f1(model = model,test_dataloader = test_dataloader,loss_fn = loss_fn)\n",
        "print(\"DONE TESTING\")\n"
      ],
      "metadata": {
        "id": "4Cts6aADSeNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 10: visualization"
      ],
      "metadata": {
        "id": "9Yt290JlSfV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the final training loss, validation loss, and generalization gap\n",
        "final_train_loss = np.sqrt(train_loss_history[-1])\n",
        "final_validation_loss = np.sqrt(validation_loss_history[-1])\n",
        "generalization_gap = final_validation_loss - final_train_loss\n",
        "\n",
        "# Plot the training loss, validation loss, and generalization gap\n",
        "plt.plot(np.sqrt(train_loss_history), label=\"Train Loss\")\n",
        "plt.plot(np.sqrt(validation_loss_history), label=\"Validation Loss\")\n",
        "\n",
        "# Add the generalization gap segment to the plot\n",
        "plt.plot(len(train_loss_history) - 1, final_train_loss, 'ro')\n",
        "plt.plot(len(validation_loss_history) - 1, final_validation_loss, 'bo')\n",
        "plt.plot([len(train_loss_history) - 1, len(validation_loss_history) - 1], [final_train_loss, final_validation_loss],\n",
        "         'k--', label=\"Generalization Gap\")\n",
        "\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JUW6AGA7Sk1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 11: saving the model"
      ],
      "metadata": {
        "id": "iUXXK5uwSwys"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYFQJjcAMwss"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = models.vgg16(pretrained=True)\n",
        "torch.save(model.state_dict(), 'BERT_weights.pth')\n",
        "\n",
        "# get a onnx copy that can be visualized with netron => https://netron.app/\n",
        "i_want_netron = False\n",
        "if i_want_netron:\n",
        "    torch.onnx.export(model, (torch.zeros(1, 128, dtype=torch.long).cuda(), torch.zeros(1, 128, dtype=torch.long).cuda()), \"bert.onnx\", input_names=[\"input_ids\", \"attention_mask\"], output_names=[\"logits\"], opset_version=11)"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "90qdjv7qCB_4",
        "kidR3CkQEAVz"
      ],
      "mount_file_id": "1MeqTa67L2M4RW75CGBkcKC0yaJKwviWk",
      "authorship_tag": "ABX9TyM17SwdsElX/YA188FRKcGl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tommasomncttn/NLP-Disaster-Tweet-Detection/blob/main/LOGISTIC_REGRESSION_NB(SKLEARN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VCvwKFmtByC7"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/ML_proj/clean_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the text and target labels from the training and testing data\n",
        "text_df = df[\"text\"]\n",
        "lable_df = df[\"target\"]\n",
        "\n",
        "# Creating a CountVectorizer object to convert the text into a matrix of token counts\n",
        "vect = CountVectorizer(stop_words='english')\n",
        "\n",
        "# Fitting the CountVectorizer on the training data to learn the vocabulary and create a document-term matrix\n",
        "vectorized_train = vect.fit_transform(text_df)\n",
        "\n",
        "# Creating a logistic regression model\n",
        "model = LogisticRegression(max_iter = 500)\n",
        "\n",
        "# Hyperparameter search\n",
        "# param_grid = {'C': np.logspace(-3, 3, 7), 'penalty': ['l2', None]}\n",
        "# model_H = GridSearchCV(model,param_grid,cv=3)"
      ],
      "metadata": {
        "id": "EQY4nsfWD5BE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of folds to use for cross-validation\n",
        "num_folds = 5\n",
        "\n",
        "# Create a KFold object to split the data into K folds\n",
        "kf = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# Create an empty list to store F1 scores for each fold\n",
        "f1_scores = []\n",
        "\n",
        "# Loop over each fold and train the model on the training data, then evaluate on the validation data\n",
        "for fold, (train_indices, val_indices) in enumerate(kf.split(vectorized_train, lable_df)):\n",
        "\n",
        "    # Split the data into training and validation sets for this fold\n",
        "    X_train, y_train = vectorized_train[train_indices], lable_df[train_indices]\n",
        "    X_val, y_val = vectorized_train[val_indices], lable_df[val_indices]\n",
        "\n",
        "    # Fit a logistic regression model on the training data\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the validation data using the trained model\n",
        "    predicted_labels = model.predict(X_val)\n",
        "\n",
        "    # Evaluate the performance of the model using F1 score\n",
        "    f1 = f1_score(predicted_labels, y_val) \n",
        "\n",
        "    # Add the F1 score for this fold to the list\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "    # Print the F1 score for this fold\n",
        "    print(f\"Fold {fold}: F1 score = {f1}\")\n",
        "\n",
        "# Calculate the average F1 score across all folds\n",
        "avg_score = np.mean(f1_scores)\n",
        "print(f\"Average F1 score across {num_folds} folds: {avg_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjJSfPgF4TRX",
        "outputId": "17c729fc-41c1-42e6-fc33-e533b30c0c9a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0: F1 score = 0.7459086993970715\n",
            "Fold 1: F1 score = 0.7750200160128103\n",
            "Fold 2: F1 score = 0.7373572593800979\n",
            "Fold 3: F1 score = 0.7436762225969645\n",
            "Fold 4: F1 score = 0.7334963325183375\n",
            "Average F1 score across 5 folds: 0.7470917059810563\n"
          ]
        }
      ]
    }
  ]
}